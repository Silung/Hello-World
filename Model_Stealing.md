相关概念：

1. DNN fingerprinting attacks
2. model inversion attacks: reveal a user’s private information in the training data leveraging model predictions

3. Model Reverse-Engineering

相关工作：

| Title                                                        | From                                                         | Year | Notes                                                        |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ---- | ------------------------------------------------------------ |
| Stealing machine learning models via prediction APIs         | Security                                                     | 2016 | 比较早的工作。黑盒设置下尝试窃取Logistic regression、Neural networks和Decision trees，设计了Equation-Solving Attacks（解方程）和Decision Tree Path-Finding Attacks，进一步尝试了在label-only设置下的攻击。 |
| Stealing neural networks via timing side channels            | arXiv                                                        | 2018 | 通过模型推理用时预测模型深度。                               |
| Security analysis of deep neural networks operating in the presence of cache side-channel attacks | arXiv                                                        | 2018 | 利用cache side-channel预测模型结构(The number and types of layers, how the layers are connected, and the activation functions)，给出了防御方法。有个作者来自Blair High School。 |
| I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences | arXiv                                                        | 2022 | Survey                                                       |
| Practical black-box attacks against machine learning         | AsiaCCS                                                      | 2017 | 由于不知道目标模型的内部结构，所以采用一个合成的数据集来训练一个本地替代模型DNN，接着采用雅可比数据增强的方式利用数据在目标模型上的输出更新数据集，再进行模型训练，以建立一个近似于oracle模型O的决策边界的模型F。最后攻击者利用替代网络F来制作对抗性样本，然后由于对抗性样本的迁移性被oracle O错误分类。 |
| Reverse engineering convolutional neural networks through side-channel information leaks | Annual Design Automation Conference                          | 2018 | 通过利用memory and timing side-channels来推断底层网络结构。偏硬件。 |
| Poisoning machine learning based wireless IDSs via stealing learning model | Wireless Algorithms, Systems, and Applications               | 2018 |                                                              |
| Stealing hyperparameters in machine learning                 | S&P                                                          | 2018 | 提出窃取模型超参数的方法，适用于各种流行的算法，如 RR、LR、SVM、NN 等。一个 ML 算法的模型参数通常对应与目标函数的最小值。这意味着模型参数的目标函数的梯度接近于 0。攻击方式为计算模型参数的目标函数梯度，并将其设置为 0，就得到了一个超参数的线性方程组（模型参数的数量一般大于未知参数的数量，位置参数如超参数），因此可通过线性最小二乘法得到近似解，从而得到超参数。 |
| Towards Reverse-Engineering Black-Box Neural Networks        | ICLR                                                         | 2018 | 黑盒设置下预测模型属性。                                     |
| Interpretability via Model Extraction                        | arXiv                                                        | 2018 | Presented as a poster at the 2017 Workshop on Fairness, Accountability, and Transparency in Machine Learning. 使用简单的模型（如决策树）近似复杂的模型，以对模型结果进行解释。 |
| Detecting Data-Free Model Stealing                           | -                                                            | 2018 | link: www.crcv.ucf.edu/wp-content/uploads/2018/11/Borum_FinalReport.pdf |
| Interpreting Blackbox Models via Model Extraction            | arXiv                                                        | 2019 | 将原始模型提取到决策树中，增强模型决策过程的可解释性。       |
| CSI NN: Reverse engineering of neural network architectures through electromagnetic side channel | Security                                                     | 2019 | 对仅包含MLP和CNN的网络模型进行重建，目标芯片为ARM Cortex-M3. 根据时间延迟(\timing delay/reaction time)、功率消耗、电磁辐射三种侧信道，还原包括激活函数、层数、每层的节点数、权重、输出的类别在内的所有参数，提出了完整的基于侧通道分析的逆向工程。 |
| Neural network model extraction attacks in edge devices by hearing architectural hints | arXiv                                                        | 2019 | Side-channel. 通过off-chip memory address traces and PCIe events准确重建网络架构。 |
| PRADA: protecting against DNN model stealing attacks         | EuroS&P                                                      | 2019 | 给出了模型窃取的通用框架，研究了模型窃取过程的两个关键步骤的几种策略。提出PRADA可以检测到所有先前的模型窃取攻击。 |
| Activethief: Model extraction using active learning and unannotated public data | AAAI                                                         | 2020 | 利用主动学习技术和未注释的公共数据集来执行模型提取。攻击逃避了最先进的模型提取检测方法PRADA的检测。 |
| Defending against neural network model stealing attacks using deceptive perturbations | SPW                                                          | 2019 |                                                              |
| Knockoff nets: Stealing functionality of black-box models    | CVPR                                                         | 2019 | 窃取CV分类模型，使用RL自适应从预定的数据集中采样构造替代数据集，提高采样效率。 |
| Extraction of complex dnn models: Real threat or boogeyman?  | Engineering Dependable and Secure Machine Learning Systems   | 2020 | 复现了Knockoff nets，并引入一种防御方法。                    |
| Prediction poisoning: Towards defenses against dnn model stealing attacks | ICLR                                                         | 2019 | 扰动防御方法。                                               |
| Model weight theft with just noise inputs: The curious case of the petulant attacker | arXiv                                                        | 2019 | 使用噪声窃取CNN的权重。                                      |
| Mimosanet: An unrobust neural network preventing model stealing | arXiv                                                        | 2019 |                                                              |
| Knowledge extraction with no observable data                 | NIPS                                                         | 2019 |                                                              |
| Model-extraction attack against FPGA-DNN accelerator utilizing correlation electromagnetic analysis | Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM) | 2019 |                                                              |
| Differentially Private Machine Learning Model against Model Extraction Attack | iThings and GreenCom and CPSCom and SmartData and Cybermatics | 2020 |                                                              |
| Model stealing defense with hybrid fuzzy models: Work-in-progress | CODES+ ISSS                                                  | 2020 |                                                              |
| Perturbing inputs to prevent model stealing                  | CNS                                                          | 2020 |                                                              |
| Causal model extraction from attack trees to attribute malicious insider attacks | GraMSec                                                      | 2020 |                                                              |
| High Accuracy and High Fidelity Extraction of Neural Networks | Security                                                     | 2020 | 感觉是不错的文章。开发了第一个无需训练的提取攻击，用于直接提取模型的权重。 |
| Practical side-channel based model extraction attack on tree-based machine learning algorithm | ACNS                                                         | 2020 |                                                              |
| A low-cost image encryption method to prevent model stealing of deep neural network | Journal of Circuits, Systems and Computers                   | 2020 |                                                              |
| Defending against model stealing attacks with adaptive misinformation | CVPR                                                         | 2020 | 通过选择性地对OOD查询发送错误的预测，大大降低了攻击者克隆模型的准确性(高达40%)，而对良性用户的准确性影响最小(< 0.5%)。 |
| Special-Purpose Model Extraction Attacks: Stealing Coarse Model with Fewer Queries | TrustCom                                                     | 2020 |                                                              |
| Prediction poisoning: Utility-constrained defenses against model stealing attacks | ICLR                                                         | 2020 |                                                              |
| Model Extraction Attacks on Recurrent Neural Networks        | Journal of Information Processing                            | 2020 | 对RNN进行模型提取攻击。                                      |
| Leaky dnn: Stealing deep-learning model secret with gpu context-switching side-channel | DSN                                                          | 2020 |                                                              |
| Cache telepathy: Leveraging shared resource attacks to learn DNN architectures | Security                                                     | 2020 | Cache side-channel获取深度神经网络结构。                     |
| Neural model stealing attack to smart mobile device on intelligent medical platform | Wireless Communications and Mobile Computing                 | 2020 |                                                              |
| DAS-AST: defending against model stealing attacks based on adaptive softmax transformation | Information Security and Cryptology                          | 2021 |                                                              |
| Tenet: A neural network model extraction attack in multi-core architecture | Proceedings of the 2021 on Great Lakes Symposium on VLSI     | 2021 |                                                              |
| Stealing links from graph neural networks                    | Security                                                     | 2021 |                                                              |
| Confident Data-free Model Stealing for Black-box Adversarial Attacks | Openreview: Under review as a conference paper at ICLR 2022  | 2021 |                                                              |
| Stealing machine learning models: Attacks and countermeasures for generative adversarial networks | ACSAC                                                        | 2021 | 针对生成对抗性网络（GANs）的模型提取攻击。                   |
| Maze: Data-free model stealing attack using zeroth-order gradient estimation | CVPR                                                         | 2021 |                                                              |
| MEGEX: Data-free model extraction attack against gradient-based explainable AI | arXiv                                                        | 2021 |                                                              |
| ActiveBoostThief: Model Extraction Attack Using Reliable Active Learning | 한국정보과학회 학술발표논문집                                | 2021 |                                                              |
| Daps: a dynamic asynchronous progress stealing model for MPI communication | IEEE International Conference on Cluster Computing (CLUSTER) | 2021 |                                                              |
| Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Generative Adversarial Networks | arXiv                                                        | 2021 |                                                              |
| First to Possess His Statistics: Data-Free Model Extraction Attack on Tabular Data | arXiv                                                        | 2021 |                                                              |
| Data-Free Model Extraction                                   | CVPR                                                         | 2021 |                                                              |
| An extraction attack on image recognition model using VAE-kdtree model | IWAIT                                                        | 2021 |                                                              |
| Beyond model extraction: Imitation attack for black-box nlp apis | arXiv                                                        | 2021 |                                                              |
| Monitoring-based differential privacy mechanism against query flooding-based model extraction attack | TDSC                                                         | 2021 |                                                              |
| Grey-box extraction of natural language models               | ICML                                                         | 2021 |                                                              |
| Thief, Beware of What Get You There: Towards Understanding Model Extraction Attack | arXiv                                                        | 2021 |                                                              |
| Stealing Deep Learning Model Secret through Remote FPGA Side-channel Analysis | A Book                                                       | 2021 |                                                              |
| Seat: similarity encoder by adversarial training for detecting model extraction attack queries | AISec                                                        | 2021 |                                                              |
| Data-free knowledge distillation for heterogeneous federated learning | ICML                                                         | 2021 |                                                              |
| Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction | RS                                                           | 2021 | 对序列推荐系统的模型窃取攻击。                               |
| Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realization | arXiv                                                        | 2021 |                                                              |
| Transformer-based Extraction of Deep Image Models            | EuroS&P                                                      | 2022 |                                                              |
| Dual Student Networks for Data-Free Model Stealing           | ICLR                                                         | 2022 |                                                              |
| SEEK: model extraction attack against hybrid secure inference protocols | arXiv                                                        | 2022 |                                                              |
| Stealing and Defending Transformer-based Encoders            | arXiv                                                        | 2022 |                                                              |
| Unsplit: Data-oblivious model inversion, model stealing, and label inference attacks against split learning | Proceedings of the 21st Workshop on Privacy in the Electronic Society | 2022 |                                                              |
| Are you stealing my model? sample correlation for fingerprinting deep neural networks | NIPS                                                         | 2022 |                                                              |
| Reconstructing training data from trained neural networks    | arXiv                                                        | 2022 |                                                              |
| CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks | arXiv                                                        | 2022 |                                                              |
| MEGA: Model Stealing via Collaborative Generator-Substitute Networks | arXiv                                                        | 2022 |                                                              |
| Model Extraction Attack against Self-supervised Speech Models | arXiv                                                        | 2022 |                                                              |
| Marich: A Query-efficient & Online Model Extraction Attack using Public Data | Under review as a conference paper at ICLR 2023              | 2022 |                                                              |
| Careful What You Wish For: on the Extraction of Adversarially Trained Models | PST                                                          | 2022 |                                                              |
| Model stealing defense against exploiting information leak through the interpretation of deep neural nets | IJCAI                                                        | 2022 |                                                              |
| Extracting Robust Models with Uncertain Examples             | ICLR                                                         | 2022 |                                                              |
| Defending against model stealing via verifying embedded external features | AAAI                                                         | 2022 |                                                              |
| Model Extraction Attack and Defense on Deep Generative Models | Journal of Physics: Conference Series                        | 2022 |                                                              |
| SeInspect: Defending Model Stealing via Heterogeneous Semantic Inspection | European Symposium on Research in Computer Security          | 2022 |                                                              |
| On the Importance of Diversity in Data-free Model Stealing   | Openreview: Under review as a conference paper at ICLR 2023  | 2022 |                                                              |
| StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning | CCS                                                          | 2022 |                                                              |
| How to steer your adversary: Targeted and efficient model stealing defenses with gradient redirection | ICML                                                         | 2022 |                                                              |
| Side-channel fuzzy analysis-based AI model extraction attack with information-theoretic perspective in intelligent IoT | IEEE Transactions on Fuzzy Systems                           | 2022 |                                                              |
| Matryoshka: Stealing Functionality of Private ML Data by Hiding Models in Model | arXiv                                                        | 2022 |                                                              |
| What can we do with just the model? A simple knowledge extraction framework | ICML                                                         | 2022 |                                                              |
| Decreasing Model Stealing Querying for Black Box Adversarial Attacks | -                                                            | 2022 | link: repository.tudelft.nl/islandora/object/uuid:8659a918-ae38-48be-acb7-565d03cf0fc0 |
| Deepsteal: Advanced model extractions leveraging efficient weight stealing in memories | S&P                                                          | 2022 |                                                              |
| Towards data-free model stealing in a hard label setting     | CVPR                                                         | 2022 |                                                              |
| Model stealing attacks against inductive graph neural networks | S&P                                                          | 2022 |                                                              |
| Effect of parameter tuning on reducing the number of queries required to perform model stealing | -                                                            | 2022 | link: repository.tudelft.nl/islandora/object/uuid:15c5a571-6721-4b32-baff-2ddf7352b726 |
| Black-box dissector: Towards erasing-based hard-label model stealing attack | ECCV                                                         | 2022 |                                                              |
| Enhance Model Stealing Attack via Label Refining             | ICSP                                                         | 2022 |                                                              |
| Dualcf: Efficient model extraction attack from counterfactual explanations | FAccT                                                        | 2022 |                                                              |
| Demystifying Arch-hints for Model Extraction: An Attack in Unified Memory System | arXiv                                                        | 2022 |                                                              |
| Model Stealing Attacks Against Vision-Language Models        | Under review as a conference paper at ICLR 2023              | 2022 |                                                              |
| A framework for understanding model extraction attack and defense | arXiv                                                        | 2022 |                                                              |
| MExMI: Pool-based Active Model Extraction Crossover Membership Inference | NIPS                                                         | 2022 |                                                              |
| GAME: Generative-Based Adaptive Model Extraction Attack      | ESORICS                                                      | 2022 |                                                              |
| Towards explainable model extraction attacks                 | International Journal of Intelligent Systems                 | 2022 |                                                              |
| Holistic Implicit Factor Evaluation of Model Extraction Attacks | TDSC                                                         | 2022 |                                                              |
| Es attack: Model stealing against deep neural networks without data hurdles | IEEE Transactions on Emerging Topics in Computational Intelligence | 2022 |                                                              |
| QEKD: query-efficient and data-free knowledge distillation from black-box models | arXiv                                                        | 2022 |                                                              |
| IDEAL: Query-Efficient Data-Free Learning from Black-Box Models | ICLR                                                         | 2022 |                                                              |
| Play the Imitation Game: Model Extraction Attack against Autonomous Driving Localization | ACSAC                                                        | 2022 |                                                              |
| I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences | arXiv                                                        | 2022 | Survey                                                       |
| DNN-Alias: Deep Neural Network Protection Against Side-Channel Attacks via Layer Balancing | arXiv                                                        | 2023 |                                                              |
| A Desynchronization-Based Countermeasure Against Side-Channel Analysis of Neural Networks | CSCML                                                        | 2023 |                                                              |
| Rethink before Releasing your Model: ML Model Extraction Attack in EDA | ASP-DAC                                                      | 2023 |                                                              |
| Perceptual Hashing of Deep Convolutional Neural Networks for Model Copy Detection | ACM Transactions on Multimedia Computing, Communications and Applications | 2023 |                                                              |
| Fast and Private Inference of Deep Neural Networks by Co-designing Activation Functions | arXiv                                                        | 2023 |                                                              |
| Honeybee: a dynamic work stealing model for mobile crowd computing | -                                                            | 2023 | link: opal.latrobe.edu.au/articles/thesis/Honeybee_a_dynamic_work_stealing_model_for_mobile_crowd_computing/21857220 |
| Scalable Scan-Chain-Based Extraction of Neural Network Models | Design, Automation & Test in Europe Conference & Exhibition (DATE) | 2023 |                                                              |
| A Comprehensive Defense Framework Against Model Extraction Attacks | TDSC                                                         | 2023 |                                                              |
| Theoretical Limits of Provable Security Against Model Extraction by Efficient Observational Defenses | SaTML                                                        | 2023 |                                                              |
| Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data | arXiv                                                        | 2023 |                                                              |
| Defending against model extraction attacks with physical unclonable function | Information Sciences                                         | 2023 |                                                              |
| Model Extraction Attacks on Split Federated Learning         | arXiv                                                        | 2023 |                                                              |
| On the feasibility of specialized ability stealing for large language code models | arXiv                                                        | 2023 |                                                              |
| ShrewdAttack: Low Cost High Accuracy Model Extraction        | Entropy                                                      | 2023 |                                                              |
| DivTheft: An Ensemble Model Stealing Attack by Divide-and-Conquer | IEEE Transactions on Dependable and Secure Computing         | 2023 |                                                              |
| Expand-and-Cluster: Exact Parameter Recovery of Neural Networks | arXiv                                                        | 2023 |                                                              |
| Model Stealing Attack against Multi-Exit Networks            | arXiv                                                        | 2023 |                                                              |
| On the Limitations of Model Stealing with Uncertainty Quantification Models | arXiv                                                        | 2023 |                                                              |
| Query-efficient model extraction for text classification model in a hard label setting | Journal of King Saud University-Computer and Information Sciences | 2023 |                                                              |
| DisGUIDE: Disagreement-Guided Data-Free Model Extraction     | AAAI                                                         | 2023 |                                                              |
| Mitigating Query-based Neural Network Fingerprinting via Data Augmentation | TOSN                                                         | 2023 |                                                              |
| Securing Distributed SGD against Gradient Leakage Threats    | TPDS                                                         | 2023 |                                                              |
| EZClone: Improving DNN Model Extraction Attack via Shape Distillation from GPU Execution Profiles | arXiv                                                        | 2023 |                                                              |
| Explanation-based data-free model extraction attacks         | WWW                                                          | 2023 |                                                              |
| Explanation leaks: Explanation-guided model extraction attacks | Information Sciences                                         | 2023 |                                                              |
| FDInet: Protecting against DNN Model Extraction via Feature Distortion Index | arXiv                                                        | 2023 |                                                              |
| Conditional generative data-free knowledge distillation      | Image and Vision Computing                                   | 2023 |                                                              |
| A Plot is Worth a Thousand Words: Model Information Stealing Attacks via Scientific Plots | arXiv                                                        | 2023 |                                                              |
| Categorical Inference Poisoning: Verifiable Defense Against Black-Box DNN Model Stealing Without Constraining Surrogate Data and Query Times | IEEE Transactions on Information Forensics and Security      | 2023 |                                                              |
| APMSA: adversarial perturbation against model stealing attacks | IEEE Transactions on Information Forensics and Security      | 2023 |                                                              |
| LibSteal: Model Extraction Attack towards Deep Learning Compilers by Reversing DNN Binary Library | ENASE                                                        | 2023 |                                                              |
| Extracting Cloud-based Model with Prior Knowledge            | arXiv                                                        | 2023 |                                                              |
|                                                              |                                                              |      |                                                              |
| 其他：                                                       |                                                              |      |                                                              |
| Adversarial Attacks Using Model Stealing                     | Poster                                                       | -    | link: cse3000-research-project.github.io/static/baeaf402808d35793f5d23116cf9b55b/poster.pdf |
| Stolen Subwords: Importance of Vocabularies for Machine Translation Model Stealing |                                                              |      | 作者：Zouhar, Vilém，没找到文章                              |
|                                                              |                                                              |      |                                                              |


